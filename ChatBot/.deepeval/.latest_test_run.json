{"testRunData": {"testCases": [{"name": "test_case_0", "input": "Generate a short, clear title for this user question:\n\"what is black hole?\"\nReturn only the title.", "actualOutput": "\"What is a Black Hole?\"", "expectedOutput": "Generate a short, clear title for this user question:\n\"what is black hole?\"\nReturn only the title.", "success": true, "metricsData": [{"name": "Answer Relevancy", "threshold": 0.7, "success": true, "score": 1.0, "reason": "The score is 1.00 because the actual output directly addresses the user's question about what a black hole is, providing a relevant and accurate response.", "strictMode": false, "evaluationModel": "llama3 (Ollama)", "evaluationCost": 0.0, "verboseLogs": "Statements:\n[\n    \"What is a Black Hole?\"\n] \n \nVerdicts:\n[\n    {\n        \"verdict\": \"yes\",\n        \"reason\": null\n    }\n]"}], "runDuration": 9.077855399998953, "evaluationCost": 0.0, "order": 0}], "conversationalTestCases": [], "metricsScores": [{"metric": "Answer Relevancy", "scores": [1.0], "passes": 1, "fails": 0, "errors": 0}], "testPassed": 1, "testFailed": 0, "runDuration": 10.142371599999024, "evaluationCost": 0.0}}